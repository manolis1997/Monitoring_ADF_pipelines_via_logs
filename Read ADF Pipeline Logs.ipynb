{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb1e03c3-c84a-49bf-ad32-659503566ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Create a diagnostic setting with the appropriate desired log's\n",
    "2. Adapt with a storage account\n",
    "3. Inside of this storage account it will be generate a container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539dca17-d368-489f-a666-7505d4a08b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.key.adlsstgeaccdevef5e74f0.dfs.core.windows.net\", \"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7be8f3-e3de-462d-8dd7-ee6222c6eeba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year\n",
    "current_month = datetime.now().month\n",
    "current_day = datetime.now().day\n",
    "\n",
    "current_month = f\"{current_month:02d}\"\n",
    "current_day = f\"{current_day:02d}\"\n",
    "print(current_year, current_month, current_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43198ca-6023-45e6-b9a1-d3232e0762aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root_path=f\"abfss://insights-logs-pipelineruns@storage_account_name.dfs.core.windows.net/resourceId=/SUBSCRIPTIONS/..../FACTORIES/MY-DEMO-ADF-DEV/y={current_year}/m={current_month}/d={current_day}/\"\n",
    "\n",
    "files = dbutils.fs.ls(root_path)\n",
    "\n",
    "json_files = [file.path for file in files if file.path.endswith('.json')]\n",
    "\n",
    "def get_json_files(path):\n",
    "    file_list = []\n",
    "    items = dbutils.fs.ls(path)\n",
    "    for item in items:\n",
    "        if item.isDir():\n",
    "            file_list.extend(get_json_files(item.path))\n",
    "        elif item.path.endswith('.json'):\n",
    "            file_list.append(item.path)\n",
    "    return file_list\n",
    "\n",
    "json_files = get_json_files(root_path)\n",
    "\n",
    "df = spark.read.json(json_files)\n",
    "\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3084822757568728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Read ADF Pipeline Logs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
